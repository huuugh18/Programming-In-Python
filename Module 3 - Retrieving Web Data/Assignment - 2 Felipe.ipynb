{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\hugho\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\hugho\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(url_1, url_2, xpath_expression):\n",
    "    def crawl(url):\n",
    "        htmlparser = etree.HTMLParser()\n",
    "        response = urlopen(url)\n",
    "        tree = etree.parse(response, htmlparser)\n",
    "        root = tree.getroot()\n",
    "        xpatheval = etree.XPathEvaluator(root)\n",
    "        content = [elem.text for elem in root.xpath(xpath_expression) if elem.text is not None]\n",
    "        text_string = ' '.join(content)\n",
    "        return text_string\n",
    "\n",
    "    def summarize(text_string):\n",
    "        #remove numbers\n",
    "        article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_string)\n",
    "        #remove white space characters\n",
    "        article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "        #remove any non letters\n",
    "        formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
    "        #remove white space again???\n",
    "        formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "        #tokenize article into sentences\n",
    "        sentence_list = nltk.sent_tokenize(article_text)\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        #create word frequencies dict\n",
    "        word_frequencies = {}\n",
    "        for word in nltk.word_tokenize(formatted_article_text):\n",
    "            if word not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "        #get max word frequency\n",
    "        maximum_frequency = max(word_frequencies.values())\n",
    "        #divide frequencies by max frequency for some reason - scale them down\n",
    "        for word in word_frequencies.keys():\n",
    "            word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "\n",
    "        #get sentence scores based on word frequency\n",
    "        # for each word in the sentence, \n",
    "        # take the word freq score and total sentence score is the sum of all the word freq scores in the sentence\n",
    "        sentence_scores = {}\n",
    "        for sent in sentence_list:\n",
    "            # tokenise the sentences - covert to all lower case\n",
    "            for word in nltk.word_tokenize(sent.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    # if less than 30 words in the sentence?\n",
    "                    if len(sent.split(' ')) < 30:\n",
    "                        if sent not in sentence_scores.keys():\n",
    "                            # add sent to dict => score = frequency score of first word\n",
    "                            sentence_scores[sent] = word_frequencies[word]\n",
    "                        else: \n",
    "                            # sent in dict => increase sent score by next word freq score\n",
    "                            sentence_scores[sent] += word_frequencies[word]\n",
    "        #get 7 largest elements from sentence_scores into a list\n",
    "        # don't know what the key part does\n",
    "        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "        # join list into one text string\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        print(f'Summary: \\n{summary}\\n')\n",
    "\n",
    "        return\n",
    "\n",
    "    def calc(content1, content2):\n",
    "        doc1 = nlp(content1)\n",
    "        doc2 = nlp(content2)\n",
    "        s = doc1.similarity(doc2)\n",
    "        print(f'Text Similarity: {s}')\n",
    "\n",
    "        return\n",
    "\n",
    "    content1 = crawl(url_1)\n",
    "    summarize(content1)\n",
    "    content2 = crawl(url_2)\n",
    "    summarize(content2)\n",
    "    result = calc(content1, content2)\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['“One thing that we can learn from these ancient stories is that sea level rise cannot be stopped very easily by sea defences like sea walls,” says Nunn.', '“I imagine that global warming and climate change and rising sea levels might inspire new geomyths,” she says.', 'Sea levels started to rise nearly 15,000 years ago with the end of the last ice age.', 'In this story the bad behaviour of a man named Goonyah caused the sea to flood the land and he then organised the people to stop it.', 'In two of the stories, the city’s elaborate defences imply the inhabitants had been fighting a losing battle against the sea for generations.', \"“I'm a conventionally trained geologist and I can tell you that a lot of other conventionally trained geoscientists really don't like this kind of thing.\", 'In another, he led the people up a mountain to escape the water where they worked together to roll heated rocks into the sea.']\n",
      "['People work long hours all over the world, for many different reasons.', '\"Workplaces can be very unhealthy environments – if there was any time to change the way we work, now is the time to do it,\" says Maslach.', '\"Burnout has cycles – like it gets rediscovered, then it dies down, and gets rediscovered again,\" says Maslach, who has studied burnout since the 1970s.', 'Overwork culture is thriving; we think of long hours and constant exhaustion as a marker of success.', 'While a lot of burnout \"culture came from Wall Street\", she says, it\\'s even worse now, because we put tech entrepreneurs who barely sleep on a pedestal.', \"They think they enter a relationship with an employer where the relationship says, 'I work hard, you take care of me'.\", '\"We dehumanised the workplace a long time ago, and I\\'m not saying it with any pride,\" says Lechner.']\n",
      "Text Similarity: 0.9850671034717985\n"
     ]
    }
   ],
   "source": [
    "url_1 = 'https://www.bbc.com/future/article/20210507-the-myths-that-hint-at-past-disasters'\n",
    "url_2 = 'https://www.bbc.com/worklife/article/20210507-why-we-glorify-the-cult-of-burnout-and-overwork'\n",
    "\n",
    "xpath_exp = './/article//div[contains(@class, \"body\")]//*'\n",
    "\n",
    "similarity(url_1, url_2, xpath_exp)\n",
    "\n"
   ]
  }
 ]
}
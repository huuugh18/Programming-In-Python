{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assigment 2\n",
    "## Complete your own implementation of a web spider. Combine it with the linguistic analysis code we saw previously (either NLP or distance matrix calculation) and use it to generate summaries of the pages. You can do this in one of two ways, either:\n",
    "\n",
    "### - Scrape a single page and then run your code on this page.\n",
    "### - Scrape a series of interlinked pages (e.g. looking for the a href element) and summarise each individual page plus a broad summary of all pages.\n",
    "\n",
    "[github repo guide]('https://medium.com/analytics-vidhya/simple-text-summarization-using-nltk-eedc36ebaaf8#:~:text=Text%20summarization%20is%20the%20process,relevant%20information%20within%20the%20Text.')\n",
    "\n",
    "[github repo guide 2]('https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/Word_Frequency_Summarization.py')\n",
    "\n",
    "**potential issue with algorithm is that long sentences will have advantage over short  \n",
    "solve this by dividing every sentence score by number of words in the sentence**\n",
    "\n",
    "** Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of the dictionary**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\hugho\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\hugho\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from heapq import nlargest\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "url_1 = 'https://en.wikipedia.org/wiki/Don_Lanphere'\n",
    "\n",
    "url_2 = 'https://en.wikipedia.org/wiki/Double-Cross_System'\n",
    "\n",
    "url_3 = 'https://en.wikipedia.org/wiki/Long_Ditton'\n",
    "\n",
    "## issue of some late paragraphs having random social media links or email or author info\n",
    "## filter out if p shorter than say 7 words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    article_soup = soup.find_all('p')\n",
    "    return article_soup\n",
    "\n",
    "def get_article_content(article_soup):\n",
    "    content = []\n",
    "    for item in article_soup:\n",
    "        content.append(item.get_text())\n",
    "    #remove shorter sentences to keep headers and non article related paragraphs out of the analysis\n",
    "    no_short_ps = []\n",
    "    for sentence in content:\n",
    "        if len(sentence) > 50:\n",
    "            no_short_pgs.append(sentence)\n",
    "        # else:\n",
    "        #     print('REMOVED SENTENCE:', sentence)\n",
    "    article_string = ' '.join(no_short_ps)\n",
    "    return article_string\n",
    "\n",
    "# get_article_content('https://en.wikipedia.org/wiki/Operation_Hardboiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(content):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    #remove non-ascii and digihts\n",
    "    content=re.sub(\"(\\\\W|\\\\d)\",\" \",content)\n",
    "    #remove whitespace\n",
    "    content = content.strip()\n",
    "    \n",
    "    tokens = word_tokenize(content)\n",
    "    word_frequencies = {}\n",
    "    for tok in tokens:\n",
    "        tok = ps.stem(tok)\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "        if tok not in word_frequencies.keys():\n",
    "            word_frequencies[tok] = 1\n",
    "        else:\n",
    "            word_frequencies[tok] += 1\n",
    "    return word_frequencies\n",
    "\n",
    "# max_word_freq = max(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_scores(sentences, freq_table) -> dict:\n",
    "    sentence_value = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        #get sentence word count\n",
    "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
    "        #set initial word count for sentence minus stop words\n",
    "        word_count_in_sentence_except_stop_words = 0\n",
    "        #create sentence value table\n",
    "        for word_val in freq_table:\n",
    "            if word_val in sentence.lower():\n",
    "                word_count_in_sentence_except_stop_words += 1\n",
    "                if sentence in sentence_value:\n",
    "                    sentence_value[sentence] += freq_table[word_val]\n",
    "                else:\n",
    "                    sentence_value[sentence] = freq_table[word_val]\n",
    "\n",
    "        if sentence in sentence_value:\n",
    "            sentence_value[sentence] = sentence_value[sentence] / word_count_in_sentence_except_stop_words\n",
    "    \n",
    "    return sentence_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_summary(sentence_scores, theshold, combined):\n",
    "    select_length = int(len(sentence_scores)*theshold)\n",
    "    if select_length > 10:\n",
    "        select_length = 10\n",
    "    if combined == True:\n",
    "        select_length = 15\n",
    "    summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
    "    \n",
    "    final_summary = ' '.join(summary)\n",
    "    \n",
    "    return final_summary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_summary_from_url(url, theshold):\n",
    "    article_soup = get_soup(url)\n",
    "\n",
    "    content_text = get_article_content(article_soup)\n",
    "\n",
    "    word_freq = get_word_freq(content_text)\n",
    "\n",
    "    sentence_tokens = sent_tokenize(content_text)\n",
    "    \n",
    "    sentence_scores = get_sentence_scores(sentence_tokens, word_freq)\n",
    "    \n",
    "    summary = get_final_summary(sentence_scores, theshold, False)\n",
    "    return summary\n",
    "\n",
    "# get_article_summary_from_url(url_1, 0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    article_body = soup.find_all('p')\n",
    "    article_links = []\n",
    "    for item in article_body:\n",
    "        children = item.findChildren('a')\n",
    "        for child in children:\n",
    "            article_links.append(child['href'])\n",
    "    article_links = article_links[:3]\n",
    "    new_links = []\n",
    "    for link in article_links:\n",
    "        new_links.append('https://en.wikipedia.org' + link)\n",
    "    new_links.append(url)\n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " SUMMARY for https://en.wikipedia.org/wiki/World_War_II: \n",
      "[294]\n",
      " Germany lost a quarter of its pre-war (1937) territory. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan. [63] The first German attack of the war came against the Polish defenses at Westerplatte. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained its territorial losses and turned towards Germany and its allies. The Americans favoured a straightforward, large-scale attack on Germany through France. [67]\n",
      " On 8 September, German troops reached the suburbs of Warsaw. A peace treaty between Japan and the Allies was signed in 1951. On 4 July 1943, Germany attacked Soviet forces around the Kursk Bulge. Both Germany and the Soviet Union used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. [94]\n",
      " On 10 June, Italy invaded France, declaring war on both France and the United Kingdom. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " SUMMARY for https://en.wikipedia.org/wiki/Counter-espionage: \n",
      "Counterintelligence is an activity aimed at protecting an agency's intelligence program from an opposition's intelligence service. [10]\n",
      " Many governments organize counterintelligence agencies separately and distinct from their intelligence collection services. Counterintelligence can both produce information and protect it. If defensive counterintelligence stops terrorist attacks, it has succeeded. Defensive counterintelligence is thwarting efforts by hostile intelligence services to penetrate the service. [7] The Secret Service Bureau was split into a foreign and counter-intelligence domestic service in 1910. Defensive counterintelligence specifically for intelligence services involves risk assessment of their culture, sources, methods and resources. For example, while offensive counterintelligence is a mission of the US CIA's National Clandestine Service, defensive counterintelligence is a mission of the U.S. Counterintelligence is part of intelligence cycle security, which, in turn, is part of intelligence cycle management. Counterespionage may involve proactive acts against foreign intelligence services, such as double agents, deception, or recruiting foreign intelligence officers. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " SUMMARY for https://en.wikipedia.org/wiki/MI5: \n",
      "[8]\n",
      " The service is directed by the Joint Intelligence Committee[9] for intelligence operational priorities. [32]\n",
      " MI5 experienced further failure during the Second World War. MI5 is directed by the Joint Intelligence Committee (JIC), and the service is bound by the Security Service Act 1989. In 1919, MI5's budget was slashed from £100,000 and over 800 officers to just £35,000 and 12 officers. It was, to some extent, a victim of its own success. [23]\n",
      " MI5 was consistently successful throughout the rest of the 1910s and 1920s in its core counter-espionage role. A high-level committee, the Wireless Board, was formed to provide this information. Agents who agreed to this were supervised by MI5 in transmitting bogus 'intelligence' back to the German secret service, the Abwehr. MI5 acquired many additional responsibilities during the war. This was the first government acknowledgement of the existence of the service. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " SUMMARY for https://en.wikipedia.org/wiki/Double-Cross_System: \n",
      "[8]\n",
      " The main form of communication that agents used with their handlers was secret writing. Agent Garbo was informed in radio messages from Germany after the invasion that he had been awarded the Iron Cross. The last route was most commonly used, with agents often impersonating refugees. It was not only in the United Kingdom that the system was operated. The policy of MI5 during the war was initially to use the system for counter-espionage. [9]:ch 25 One of the agents sent genuine information about Operation Torch to the Germans. The Germans became dependent on the spurious information that was fed to them by Garbo's network and the other double-cross agents. [10][11] Duncan Sandys was told to get MI5-controlled German agents such as Zig Zag and Tate to report the V-1 impacts back to Germany. After the war, it was discovered that all the agents Germany sent to Britain had given themselves up or had been captured, with the possible exception of one who committed suicide. Later in the war, wireless sets were provided by the Germans. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " COMBINED ARTICLE SUMMARY: \n",
      "[294]\n",
      " Germany lost a quarter of its pre-war (1937) territory. The policy of MI5 during the war was initially to use the system for counter-espionage. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan. It was not only in the United Kingdom that the system was operated. [123]\n",
      " Hitler believed that the United Kingdom's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later. Both Germany and the Soviet Union used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. [63] The first German attack of the war came against the Polish defenses at Westerplatte. [67]\n",
      " On 8 September, German troops reached the suburbs of Warsaw. Later in the war, wireless sets were provided by the Germans. The Soviet Union alone lost around 27 million people during the war,[341] including 8.7 million military and 19 million civilian deaths. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained its territorial losses and turned towards Germany and its allies. The Americans favoured a straightforward, large-scale attack on Germany through France. [314]\n",
      " Post-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact. [318]\n",
      " In China, nationalist and communist forces resumed the civil war in June 1946. The war in Europe concluded with the liberation of German-occupied territories, and the invasion of Germany by the Western Allies and the Soviet Union, culminating in the fall of Berlin to Soviet troops, Hitler's suicide and the German unconditional surrender on 8 May 1945. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_linked_article_summaries(url):\n",
    "    link_list = get_article_links(url)\n",
    "    summaries = []\n",
    "    for article_url in link_list:\n",
    "        summary = get_article_summary_from_url(article_url, 0.2)\n",
    "        summaries.append(summary)\n",
    "        print(f'\\n SUMMARY for {article_url}: \\n{ summary } \\n\\n\\n')\n",
    "    combined_summary = get_multiple_article_summary(link_list, 0.4)\n",
    "    print(f'\\n COMBINED ARTICLE SUMMARY: \\n{ combined_summary } \\n\\n\\n')\n",
    "    \n",
    "\n",
    "def get_multiple_article_summary(url_list, threshold):\n",
    "    big_ole_soup = []\n",
    "        \n",
    "    for url in url_list:\n",
    "        url_soup = get_soup(url)\n",
    "        contents = get_article_content(url_soup)\n",
    "        big_ole_soup.append(contents)\n",
    "    \n",
    "    content_text = ' '.join(big_ole_soup)\n",
    "\n",
    "    word_freq = get_word_freq(content_text)\n",
    "    \n",
    "    sentence_tokens = sent_tokenize(content_text)\n",
    "    \n",
    "    sentence_scores = get_sentence_scores(sentence_tokens, word_freq)\n",
    "    \n",
    "    summary = get_final_summary(sentence_scores, threshold, True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "get_linked_article_summaries(url_2)\n",
    "\n",
    "#issue where longer articles have heavier weightinig and they dominate the results\n",
    "# would have to adjust this to weight the articles based on size somehow similar to weighting sentence by word count"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Write a snippet of code to retrieve data from a web page utilising the Document Object Model (DOM.) This could be a simple example such as grabbing the 3rd `<p>` tag or all the alt text from a series of images. Post your code into the 3.2 Discussion forum and comment on at least two other code snippets suggesting changes or improvements. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "[FCC Webscraping with Selenium / BS / Pandas]('https://www.freecodecamp.org/news/better-web-scraping-in-python-with-selenium-beautiful-soup-and-pandas-d6390592e251/')\n",
    "\n",
    "## The Selenium package is used to automate web browser interaction from Python. With Selenium, programming a Python script to automate a web browser is possible. Afterwards, those pesky JavaScript links are no longer an issue.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "source": [
    "## Selenium will now start a browser session. For Selenium to work, it must access the browser driver. By default, it will look in the same directory as the Python script. Links to Chrome, Firefox, Edge, and Safari drivers available here. The example code below uses Firefox:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch url\n",
    "url = \"http://kanview.ks.gov/PayRates/PayRates_Agency.aspx\"\n",
    "\n",
    "# create a new Firefox session\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(url)\n",
    "\n",
    "python_button = driver.find_element_by_id('MainContent_uxLevel1_Agencies_uxAgencyBtn_33') #FHSU\n",
    "python_button.click() #click fhsu link\n",
    "\n"
   ]
  },
  {
   "source": [
    "## The `python_button.click()` above is telling Selenium to click the JavaScript link on the page. After arriving at the Job Titles page, Selenium hands off the page source to Beautiful Soup."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium hands the page source to Beautiful Soup\n",
    "soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "datalist = [] #empty list\n",
    "x = 0 #counter\n",
    "\n",
    "for link in soup_level1.find_all('a', id=re.compile(\"^MainContent_uxLevel2_JobTitles_uxJobTitleBtn_\")):\n",
    "    #Selenium visits each Job Title page\n",
    "    python_button = driver.find_element_by_id('MainContent_uxLevel2_JobTitles_uxJobTitleBtn_' + str(x))\n",
    "    python_button.click() #click link\n",
    "    \n",
    "    #Selenium hands of the source of the specific job page to Beautiful Soup\n",
    "    soup_level2=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    #Beautiful Soup grabs the HTML table on the page\n",
    "    table = soup_level2.find_all('table')[0]\n",
    "    \n",
    "    #Giving the HTML table to pandas to put in a dataframe object\n",
    "    df = pd.read_html(str(table),header=0)\n",
    "    \n",
    "    #Store the dataframe in a list\n",
    "    datalist.append(df[0])\n",
    "    \n",
    "    #Ask Selenium to click the back button\n",
    "    driver.execute_script(\"window.history.go(-1)\") \n",
    "    \n",
    "    #increment the counter variable before starting the loop over\n",
    "    x += 1"
   ]
  },
  {
   "source": [
    "Pandas uses its read_html function to read the HTML table data into a dataframe. The dataframe is appended to the previously defined empty list.\n",
    "\n",
    "Before the code block of the loop is complete, Selenium needs to click the back button in the browser.\n",
    "\n",
    "Once loop complete - BS has got table from every page - pandas stored table in a DF - each DF stored in `datalist`\n",
    "\n",
    "Now merge individual DFs into one large DF\n",
    "\n",
    "Then convert to JSON and python creates JSON data file\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end the Selenium browser session\n",
    "driver.quit()\n",
    "\n",
    "#combine all pandas dataframes in the list into one big dataframe\n",
    "result = pd.concat([pd.DataFrame(datalist[i]) for i in range(len(datalist))],ignore_index=True)\n",
    "\n",
    "#convert the pandas dataframe to JSON\n",
    "json_records = result.to_json(orient='records')\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "#open, write, and close the file\n",
    "f = open(path + \"\\\\fhsu_payroll_data.json\",\"w\") #FHSU\n",
    "f.write(json_records)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "## Another Selenium Example from FCC\n",
    "## [using selenium and python to scrape websites]('https://www.freecodecamp.org/news/improve-web-scraping-with-selenium/')\n",
    "\n",
    "## Basic webscraping with BS and Pandas\n",
    "## [Scraping an Ecommerce Site]('https://www.freecodecamp.org/news/scraping-ecommerce-website-with-python/')\n",
    "\n",
    "Also, we will send a user-agent on every HTTP request, because if you make GET request using requests then by default the user-agent is Python which might get blocked.\n",
    "\n",
    "So, to override that, we will declare a variable which will store our user-agent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "baseurl = \"https://www.thewhiskyexchange.com\"\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}\n",
    "productlinks = []\n",
    "t={}\n",
    "data=[]\n",
    "c=0\n",
    "for x in range(1,6):\n",
    "    k = requests.get('https://www.thewhiskyexchange.com/c/35/japanese-whisky?pg={}&psize=24&sort=pasc'.format(x)).text\n",
    "    soup=BeautifulSoup(k,'html.parser')\n",
    "    productlist = soup.find_all(\"li\",{\"class\":\"product-grid__item\"})\n",
    "\n",
    "\n",
    "    for product in productlist:\n",
    "        link = product.find(\"a\",{\"class\":\"product-card\"}).get('href')\n",
    "        productlinks.append(baseurl + link)\n",
    "\n",
    "\n",
    "for link in productlinks:\n",
    "    f = requests.get(link,headers=headers).text\n",
    "    hun=BeautifulSoup(f,'html.parser')\n",
    "\n",
    "    try:\n",
    "        price=hun.find(\"p\",{\"class\":\"product-action__price\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        price = None\n",
    "\n",
    "    try:\n",
    "        about=hun.find(\"div\",{\"class\":\"product-main__description\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        about=None\n",
    "\n",
    "    try:\n",
    "        rating = hun.find(\"div\",{\"class\":\"review-overview\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        rating=None\n",
    "\n",
    "    try:\n",
    "        name=hun.find(\"h1\",{\"class\":\"product-main__name\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        name=None\n",
    "\n",
    "    whisky = {\"name\":name,\"price\":price,\"rating\":rating,\"about\":about}\n",
    "\n",
    "    data.append(whisky)\n",
    "    c=c+1\n",
    "    print(\"completed\",c)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  }
 ]
}
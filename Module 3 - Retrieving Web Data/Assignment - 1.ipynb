{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try downloading some web pages using a Python program and extracting information. Look at the page in your web browser and use the inspector to locate areas of interest.  \n",
    "\n",
    "Refer to a library like beautifulsoup4 or pyquery documentation to find out how to search the HTML for more specific things\n",
    "\n",
    "For example, tags with particular class attributes.\n",
    "\n",
    "Look at CFL punting and kick off stats for 2019\n",
    "\n",
    "[Punt and Kick Stats - https://www.cfl.ca/stats/?stat_category=punting&season=2019](https://www.cfl.ca/stats/?stat_category=punting&season=2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Date', 'NAME', 'URL', 'Team', 'GP', 'PUNTS', 'YDS', 'AVG', 'LG', 'S', 'KICKOFFS', 'YDS', 'AVG', 'LG', 'S']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "season = '2019'\n",
    "url = 'https://www.cfl.ca/stats/?stat_category=punting&season=' + season\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "table_head = soup.find_all('th', attrs={'class': 'cell-th'})\n",
    "\n",
    "col_headers = []\n",
    "\n",
    "for th in table_head:\n",
    "    col_headers.append(th.text)\n",
    "\n",
    "#insert extra column heading that appears in data\n",
    "col_headers.insert(2, 'URL')\n",
    "\n",
    "print(col_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying to import all the data at once using the request I realized that the data is actually loaded seperately. The page is not static.  \n",
    "I was able to locate the data in the network tab of the dev tools and import that as json. I needed to import the headers seperately as those could be loaded straight from the url for the webpage. However thet data for the column headers was one less than the data in the main table as the json data had an extra category of the url to link the player to their player page so I added that into the `col_headers` but then dropped it after creating the dataframe as I thought this the eiasier course of action instead of filtering the data before dataframe creation. End result is I have successfully webscraped the table from the website into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Date               NAME Team  GP  PUNTS   YDS   AVG  LG   S  KICKOFFS  \\\n",
       "0  2019      LEONE, Richie  OTT  18    132  6383  48.4  77   6        50   \n",
       "1  2019     RYAN, Jonathan  SSK  18    107  5222  48.8  77  12         0   \n",
       "2  2019        BEDE, Boris  MTL  18    109  4862  44.6  61   2        83   \n",
       "3  2019    MEDLOCK, Justin  WPG  18    106  4716  44.5  71   1        79   \n",
       "4  2019  HAJRULLAHU, Lirim  HAM  18    106  4566  43.1  62   1        91   \n",
       "\n",
       "    YDS   AVG  LG  S  \n",
       "0  3091  61.8  72  1  \n",
       "1     0   0.0   0  0  \n",
       "2  5772  69.5  95  5  \n",
       "3  5310  67.2  85  2  \n",
       "4  5701  62.6  80  1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>NAME</th>\n      <th>Team</th>\n      <th>GP</th>\n      <th>PUNTS</th>\n      <th>YDS</th>\n      <th>AVG</th>\n      <th>LG</th>\n      <th>S</th>\n      <th>KICKOFFS</th>\n      <th>YDS</th>\n      <th>AVG</th>\n      <th>LG</th>\n      <th>S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019</td>\n      <td>LEONE, Richie</td>\n      <td>OTT</td>\n      <td>18</td>\n      <td>132</td>\n      <td>6383</td>\n      <td>48.4</td>\n      <td>77</td>\n      <td>6</td>\n      <td>50</td>\n      <td>3091</td>\n      <td>61.8</td>\n      <td>72</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019</td>\n      <td>RYAN, Jonathan</td>\n      <td>SSK</td>\n      <td>18</td>\n      <td>107</td>\n      <td>5222</td>\n      <td>48.8</td>\n      <td>77</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019</td>\n      <td>BEDE, Boris</td>\n      <td>MTL</td>\n      <td>18</td>\n      <td>109</td>\n      <td>4862</td>\n      <td>44.6</td>\n      <td>61</td>\n      <td>2</td>\n      <td>83</td>\n      <td>5772</td>\n      <td>69.5</td>\n      <td>95</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019</td>\n      <td>MEDLOCK, Justin</td>\n      <td>WPG</td>\n      <td>18</td>\n      <td>106</td>\n      <td>4716</td>\n      <td>44.5</td>\n      <td>71</td>\n      <td>1</td>\n      <td>79</td>\n      <td>5310</td>\n      <td>67.2</td>\n      <td>85</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019</td>\n      <td>HAJRULLAHU, Lirim</td>\n      <td>HAM</td>\n      <td>18</td>\n      <td>106</td>\n      <td>4566</td>\n      <td>43.1</td>\n      <td>62</td>\n      <td>1</td>\n      <td>91</td>\n      <td>5701</td>\n      <td>62.6</td>\n      <td>80</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_page = requests.get('https://www.cfl.ca/wp-content/themes/cfl.ca/inc/admin-ajax.php?action=get_league_stats&stat_category=punting&season=2019')\n",
    "\n",
    "soup = BeautifulSoup(data_page.content, 'html.parser')\n",
    "soup.text\n",
    "site_json=json.loads(soup.text)\n",
    "\n",
    "site_json['data'][2]\n",
    "\n",
    "player_data = []\n",
    "\n",
    "for row in site_json['data']:\n",
    "    player_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(player_data, columns=col_headers)\n",
    "df = df.drop(['URL'], axis=1)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDIF Vectorizer\n",
    "[TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "Equivalent to CountVectorizer followed by TfidfTransformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_tf_idf(doc_path):\n",
    "    vectorizer =TfidfVectorizer(use_idf=True)\n",
    "    corpus = []\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        corpus = f.readlines()\n",
    "\n",
    "    # tfidf = vectorizer.fit_transform(corpus).toarray()\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    #get first vector from first doc\n",
    "    first_vector_tfidf = tfidf[0]\n",
    "\n",
    "    df = pd.DataFrame(first_vector_tfidf.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    return df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "    \n",
    "# get_tf_idf(\"Data/oldmanandthesea.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "\n",
    "CountVectorizer - transform a corpora of text to a vector of term / token counts. It also provides the capability to preprocess your text data prior to generating the vector representation making it a highly flexible feature representation module for text.\n",
    "\n",
    "[How to use CountVectorizer](https://kavita-ganesan.com/how-to-use-countvectorizer/)\n",
    "\n",
    "\n",
    "By default, CountVectorizer does the following:\n",
    "\n",
    "- lowercases your text (set lowercase=false if you don’t want lowercasing)\n",
    "- uses utf-8 encoding\n",
    "- performs tokenization (converts raw text to smaller units of text)\n",
    "- uses word level tokenization (meaning each word is treated as a separate token)\n",
    "- ignores single characters during tokenization (say goodbye to words like ‘a’ and ‘I’)\n",
    "\n",
    "### Stop Words\n",
    "Can add custom stop words as parameter\n",
    "` stop_words=['all', 'and', the']`\n",
    "\n",
    "Can check stop words being used with `cv.stop_words` and `cv.stop_words_` - Check example below\n",
    "\n",
    "### Min_df and Max_df (Document Frequency)\n",
    "*How many Documents contained a term*\n",
    "\n",
    "goal of min_df to ignore words with too few occurances to be meaningful\n",
    "\n",
    "Can be absolute value (eg 1, 2, 3) or proportion (0.25)\n",
    "\n",
    "Max_DF - Remove words that are too common - Typically use 0.75 - 0.85\n",
    "\n",
    "### *Why Doc Frequency for eliminating words?*\n",
    "Term frequency can be misleading\n",
    "Ex. say 1 document in 250 contains a word 'firetruck' 500 times\n",
    "\n",
    "\n",
    "### Custom Tokenization\n",
    "pass in parameter of a function `cv = CountVectorizer(dataDocs, tokenizer=my_tokenizer)`\n",
    "\n",
    "\n",
    "### Custom Preprocessing\n",
    "Preprocessing helps reduce noise and improves sparsity issues => more accurate analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': 34, 'cent': 14, ',': 4, 'two': 47, 'cents': 15, 'old': 32, 'new': 29, ':': 5, 'all': 7, 'about': 6, 'money': 28, '(': 2, 'cat': 13, 'in': 22, 'the': 44, 'hat': 19, \"'\": 1, 's': 38, 'learning': 25, 'library': 26, 'inside': 24, 'your': 49, 'outside': 36, 'human': 21, 'body': 10, ')': 3, '': 0, 'oh': 31, 'things': 46, 'you': 48, 'can': 12, 'do': 16, 'that': 43, 'are': 8, 'good': 18, 'for': 17, 'staying': 41, 'healthy': 20, 'on': 33, 'beyond': 9, 'bugs': 11, 'insects': 23, 'there': 45, 'no': 30, 'place': 37, 'like': 27, 'space': 40, 'our': 35, 'solar': 39, 'system': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cat_in_the_hat_docs=[\n",
    "    \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "    \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "    \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "    \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "    \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
    "]\n",
    "\n",
    "cv = CountVectorizer(cat_in_the_hat_docs)\n",
    "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
    "\n",
    "# Show resulting vocab\n",
    "cv.vocabulary_\n",
    "\n",
    "# shape of count vector: 5 docs (book titles) and 43 unique words\n",
    "count_vector.shape\n",
    "\n",
    "# --- STOP WORDS --- \n",
    "\n",
    "# CUSTOM STOP WORD LIST\n",
    "cv = CountVectorizer(cat_in_the_hat_docs,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"], min_df=2)\n",
    "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
    "count_vector.shape\n",
    "\n",
    "# CHECK STOP WORDS BEING USED when explicitly specified\n",
    "cv.stop_words\n",
    "\n",
    "\n",
    "# MIN_DF AND MAX DF\n",
    "# cv = CountVectorizer(cat_in_the_hat_docs,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"], min_df=2)\n",
    "cv = CountVectorizer(cat_in_the_hat_docs,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"], max_df=0.85)\n",
    "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
    "count_vector.shape\n",
    "\n",
    "# CHECK STOP WORDS inferred from min_df and max_df (data frequency)\n",
    "cv.stop_words_\n",
    "\n",
    "\n",
    "# --- CUSTOM TOKENIZATION ---\n",
    "import re\n",
    "\n",
    "# keep punctuation \n",
    "def my_tokenizer(text):\n",
    "    #create space b/w characters\n",
    "    text=re.sub(\"(\\\\W)\",\" \\\\1 \",text)\n",
    "\n",
    "    # split based on whitespace\n",
    "    return re.split(\"\\\\s+\",text)\n",
    "\n",
    "cv = CountVectorizer(cat_in_the_hat_docs,tokenizer=my_tokenizer)\n",
    "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': 25, 'cent': 8, 'two': 37, 'old': 23, 'new': 20, '_connector_': 0, 'about': 1, 'money': 19, 'cat': 7, 'the': 34, 'hat': 11, 'learn': 16, 'librari': 17, 'insid': 15, 'your': 39, 'outsid': 27, 'human': 13, 'bodi': 4, 'oh': 22, 'thing': 36, 'you': 38, 'can': 6, 'do': 9, 'that': 33, 'are': 2, 'good': 10, 'stay': 31, 'healthi': 12, 'on': 24, 'beyond': 3, 'bug': 5, 'insect': 14, 'there': 35, 'no': 21, 'place': 28, 'like': 18, 'space': 30, 'our': 26, 'solar': 29, 'system': 32}\n"
     ]
    }
   ],
   "source": [
    "# ----- CUSTOM PREPROCESSING WITH PORTERSTEMMER -----\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "def my_cool_preprocessor(text):\n",
    "    \n",
    "    text=text.lower() #lowercase text (done be default if don't use a custom preprocessor)\n",
    "    text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "    \n",
    "    # stem words\n",
    "    words=re.split(\"\\\\s+\",text)\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "cv = CountVectorizer(cat_in_the_hat_docs,preprocessor=my_cool_preprocessor)\n",
    "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on article from \n",
    "[Measuring Similarity Between Texts in Python](https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF MATRIX: [[0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0]\n",
      " [1 1 1 2 0 1 0 0 2 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
      "  0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 2 0 0 0 1\n",
      "  1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1]]\n",
      "TF IDF MATRIX: [[0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.37796447 0.         0.\n",
      "  0.         0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.        ]\n",
      " [0.19381304 0.19381304 0.19381304 0.38762607 0.         0.19381304\n",
      "  0.         0.         0.38762607 0.         0.         0.19381304\n",
      "  0.         0.         0.         0.         0.         0.15280442\n",
      "  0.19381304 0.19381304 0.19381304 0.         0.19381304 0.19381304\n",
      "  0.         0.         0.19381304 0.         0.19381304 0.19381304\n",
      "  0.         0.         0.19381304 0.19381304 0.19381304 0.\n",
      "  0.         0.         0.19381304 0.19381304 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27094807 0.         0.         0.27094807 0.\n",
      "  0.         0.27094807 0.         0.         0.27094807 0.21361857\n",
      "  0.         0.         0.         0.27094807 0.         0.\n",
      "  0.27094807 0.         0.         0.         0.         0.\n",
      "  0.27094807 0.54189613 0.         0.         0.         0.27094807\n",
      "  0.27094807 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.57735027 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.57735027]]\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         0.03264186 0.        ]\n",
      " [0.         0.03264186 1.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugho\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-14c82d2b3263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m#get euclidian distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mdistance_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\kdtree.py\u001b[0m in \u001b[0;36mdistance_matrix\u001b[1;34m(x, y, p, threshold)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "d1 = \"plot: two teen couples go to a church party, drink and then drive.\"\n",
    "d2 = \"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \"\n",
    "d3 = \"every now and then a movie comes along from a suspect studio , with every indication that it will be a stinker , and to everybody's surprise ( perhaps even the studio ) the film becomes a critical darling . \"\n",
    "d4 = \"damn that y2k bug . \"\n",
    "documents = [d1, d2, d3, d4]\n",
    "\n",
    "\n",
    "\n",
    "import nltk, string, numpy\n",
    "\n",
    "#Normalize by lemmatization\n",
    "# nltk.download('wordnet')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return[lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# Turn text into vectors of term freq\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(documents)\n",
    "\n",
    "# print(LemVectorizer.vocabulary_)\n",
    "\n",
    "tf_matrix = LemVectorizer.transform(documents).toarray()\n",
    "print('TF MATRIX:', tf_matrix)\n",
    "\n",
    "#get idf - convert tf matrix to tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "# Here what the transform method does is multiplying the tf matrix (4 by 41) by the diagonal idf matrix (41 by 41 with idf for each term on the main diagonal), \n",
    "# and dividing the tf-idf by the Euclidean norm. This output takes too much space and you can check it by yourself.\n",
    "# print( tfidfTran.idf_)\n",
    "\n",
    "#get tf-idf matrix (4x41)\n",
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print('TF IDF MATRIX:',tfidf_matrix.toarray())\n",
    "\n",
    "# cos similarity matrix\n",
    "cos_similarity_matrix = (tfidf_matrix*tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)\n",
    "\n",
    "#get euclidian distances\n",
    "from scipy.spatial import distance_matrix\n",
    "distance_matrix(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CH. 6 - Data loading, storage, and file formats\n",
    "\n",
    "Pandas parsing functions for different data formats optional arguments:\n",
    "- Indexing: treat one or more col's as the returend df and whether to get col names from the file, user, or not at all\n",
    "- Type Inference and Data Conversion:\n",
    "    - includes user-defined value conversions and custom list of missing value markers\n",
    "- Datetime Parsing:\n",
    "    - includes combining capability - can combine date and time info spread over multiple cols\n",
    "- Iterating:\n",
    "    - support for iterating over chunks of very large files\n",
    "- Unclean Data Issues:\n",
    "    - skip rows or a footer, comments and other minor things like numbers seperated with commas\n",
    "\n",
    "Because of how complex real world data can be, there are a large amount of different parameters when reading in files\n",
    "\n",
    "\n",
    "\n",
    "If no header in file - skip header and let pd assign headers  \n",
    "`pd.read_csv('somefile.csv', header=None)`\n",
    "\n",
    "Set your own headers  \n",
    "`pd.read_csv('somefile.csv', names=['a', 'b', 'c'])  \n",
    "\n",
    "Set index from a column  \n",
    "`names = ['a', 'b', 'c']`  \n",
    "`pd.read_csv('file.csv', names=names, index_col='c')`\n",
    "\n",
    "Hierarchical index from multiple columns - pass a list of col numbers or names (pp. 172)  \n",
    "`parsed = pd.read_csv('file.csv', index_col=['key1', 'key2'])`\n",
    "\n",
    "Specify a whitespace delimmeter  \n",
    "`result = pd.read_csv('examples/ex3.txt', sep='\\s+')`\n",
    "\n",
    "Skip rows in data:  \n",
    "`pd.read_csv('data.csv', skiprows=[0, 2, 3])`  \n",
    "\n",
    "Specify null values in data  \n",
    "`pd.read_csv('data.csv', na_values=['NULL'])`\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Reading Text Files in Pieces\n",
    "\n",
    "if dealing with large file may only want to read a small piece or do chunks\n",
    "\n",
    "Set pd display settings more compact  \n",
    "`pd.options.display.max_rows = 10`\n",
    "\n",
    "Read small number of rows  \n",
    "pd.read_csv('data.csv', nrows=5)\n",
    "\n",
    "Set a `chunksize` as a number of rows  \n",
    "`chunker = pd.read_csv('data.csv', chunksize=1000)`\n",
    "\n",
    "Export data to csv  \n",
    "`data.to_csv('filepath/data.csv')`\n",
    "\n",
    "disable row and column labels  \n",
    "`data.to_csv('filepath/data.csv', index=False, header=False)`\n",
    "\n",
    "choose subset of coulumns  \n",
    "`data.to_csv('filepath/data.csv', index=False, columns=['a', 'b'])`\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# JSON Data\n",
    "\n",
    "Convert json object into python form\n",
    "`import json`  \n",
    "`result = json.loads(obj)`  \n",
    "\n",
    "Convert back to json  \n",
    "`asjson = json.dumps(result)`  \n",
    "\n",
    "Pass a list of dicts to a df  \n",
    "`siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n",
    "\n",
    "`pandas.read_json` can auto convert JSON in specific arrangements to series or df  \n",
    "assumes that each object in the JSON array is a row in the table\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Web Scraping HTML and XML\n",
    "\n",
    "`pandas.read_html` - auto parse tables out of HTML files to df objects\n",
    "\n",
    "## Parse XML with `lxml.objectify`\n",
    "\n",
    "`from lxml import objectify`  \n",
    "`path = 'some/file/path.xml`  \n",
    "`parsed = objectify.parse(open(path))`  \n",
    "`root = parsed.getroot()` - get to the root node of the XML file  \n",
    "\n",
    "`root.INDICATOR` returns a generator yeilding each XML element - for each element we can populate a dict of tag names to data values  \n",
    "\n",
    "`skip_fields = ['a', 'd']`\n",
    "\n",
    "`for elt in root.INDICATOR:`  \n",
    "    `el_data = {}`\n",
    "    `for child in elt.getChildren():`  \n",
    "        `if child.tag in skip_fields:`  \n",
    "            `continue`  \n",
    "        `el_data[child_tag] = child.pyval`  \n",
    "    `data.append(el_data)`  \n",
    "\n",
    "\n",
    "   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6.2 Binary Data Formats\n",
    "\n",
    "store data in binary format using python's built in `pickle` serialization  \n",
    "pandas objects all have a `to_pickle` method that writes thet data to a disk in pickle format:\n",
    "\n",
    "`df.to_pickle('path/pickledata')`  \n",
    "\n",
    "Read pickled data  \n",
    "`pd.read_pickle('path/pickledata')`\n",
    "\n",
    "## HDF5 Format\n",
    "\n",
    "intended for storing large quantities of scientific array data  \n",
    "can store multiple datasets and support metadata\n",
    "\n",
    "good choice for very large datasets - can efficiently read and write small sections of much larger arrays\n",
    "\n",
    "## Excel Files\n",
    "must use add on packages `xlrd` and `openpyxl` to read XLS and XLSX files respectively\n",
    "\n",
    "`xlsx = pd.ExcelFile('file.xlsx')`  \n",
    "`pd.read_excel(xlsx, 'Sheet1)`\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6.3 Web APIs\n",
    "\n",
    "`import requests`  \n",
    "`url = 'someurl.com/api_stuff`  \n",
    "`resp = requests.get(url)`  \n",
    "\n",
    "JSON method return a dict containing JSON parsed into native python objs  \n",
    "`data = resp.json()`  \n",
    "`data[0]['title']`  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6.4  Databases\n",
    "\n",
    "loading data from SQL into a df is fairly straightforward\n",
    "\n",
    "create a SQLite db using `sqlite3` driver:  \n",
    "`import sqlite3`  \n",
    "`query = \"\"\"`\n",
    "`CREATE TABLE test`  \n",
    "`(a VARCHAR(20), b VARCHAR(20)`  \n",
    "` c REAL, d INTEGER`  \n",
    "`); \"\"\"`\n",
    "\n",
    "`con = sqlite3.connect('mydata.sqlite')` "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}